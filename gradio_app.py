#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import gradio as gr
import os
import json
import tempfile
import shutil
from pathlib import Path
import torch
import sys
from typing import Optional, Tuple, List
import numpy as np
from PIL import Image
import mediapy
import time
import time
import traceback
import gc

# æ·»åŠ é¡¹ç›®è·¯å¾„
path_to_insert = "humo"
if path_to_insert not in sys.path:
    sys.path.insert(0, path_to_insert)

from common.config import load_config, create_object
from common.distributed import get_device, get_global_rank, init_torch
from common.logger import get_logger
from humo.models.utils.utils import tensor_to_video

# è§†é¢‘å°ºå¯¸é…ç½®
SIZE_CONFIGS = {
    '720*1280': (720, 1280),
    '1280*720': (1280, 720),
    '480*832': (480, 832),
    '832*480': (832, 480),
    '1024*1024': (1024, 1024),
}

class HuMoGradioApp:
    def __init__(self):
        self.generator = None
        self.config = None
        self.temp_dir = tempfile.mkdtemp()
        self.progress = 0
        self.is_generating = False
        self.model_type = None  # è®°å½•å½“å‰æ¨¡å‹ç±»å‹
        self.logger = get_logger(self.__class__.__name__)
        
    def load_model(self, model_type: str = "1.7B"):
        """åŠ è½½HuMoæ¨¡å‹"""
        try:
            # å…ˆé‡Šæ”¾æ˜¾å­˜
            gc.collect()
            torch.cuda.empty_cache()

            # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®è·¯å¾„å’Œé…ç½®
            if model_type == "1.7B":
                model_path = "./weights/HuMo/HuMo-1.7B"
                config_path = "humo/configs/inference/generate_1_7B.yaml"
                self.model_type = "1.7B"
            elif model_type == "17B":
                model_path = "./weights/HuMo/HuMo-17B"
                config_path = "humo/configs/inference/generate.yaml"
                self.model_type = "17B"
            else:
                return f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}"
            
            if not os.path.exists(model_path):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
            
            if not os.path.exists(config_path):
                return f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}"
            
            # åŠ è½½é…ç½®
            self.config = load_config(config_path, [])
            self.config.dit.checkpoint_dir = model_path
            
            # åœ¨å•æœºæ¨¡å¼ä¸‹ç¦ç”¨åºåˆ—å¹¶è¡Œ
            import torch.distributed as dist
            if not dist.is_initialized() or dist.get_world_size() == 1:
                self.config.dit.sp_size = 1
                self.config.generation.sequence_parallel = 1
                print("ğŸ”§ å•æœºæ¨¡å¼ï¼šå·²ç¦ç”¨åºåˆ—å¹¶è¡Œ")
            
            # åˆå§‹åŒ–torch
            init_torch(cudnn_benchmark=False)
            
            # åˆ›å»ºç”Ÿæˆå™¨
            self.generator = create_object(self.config)

            # é…ç½®æ¨¡å‹ç»„ä»¶ï¼ˆåŒ…æ‹¬vaeï¼‰
            self.generator.configure_models()
            
            return f"âœ… {model_type}æ¨¡å‹åŠ è½½æˆåŠŸï¼"
        except Exception as e:
            traceback.print_exc()
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def update_progress(self) -> float:
        """æ›´æ–°è¿›åº¦æ¡"""
        if not self.is_generating:
            return 0
        self.progress += 1
        # æ¨¡æ‹Ÿè¿›åº¦ï¼Œæœ€å¤§å€¼ä¸º99%ï¼Œç•™1%ç»™æœ€ç»ˆå¤„ç†
        progress_value = min(99, self.progress)
        return progress_value
    
    def generate_video(
        self,
        prompt: str,
        negative_prompt: str = "",
        ref_image: Optional[Image.Image] = None,
        audio_file: Optional[str] = None,
        mode: str = "TA",
        frames: int = 97,
        height: int = 720,
        width: int = 1280,
        scale_i: float = 5.0,
        scale_a: float = 5.5,
        scale_t: float = 5.0,
        sampling_steps: int = 50,
        seed: int = 666666,
        fps: int = 25
    ) -> Tuple[str, str, float]:
        """ç”Ÿæˆè§†é¢‘"""
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # é‡ç½®è¿›åº¦
            self.progress = 0
            self.is_generating = True
            
            if self.generator is None:
                self.is_generating = False
                return "", "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", 0
            
            if not prompt.strip():
                self.is_generating = False
                return "", "âŒ è¯·è¾“å…¥æ–‡æœ¬æç¤º", 0
            
            # å‡†å¤‡è¾“å‡ºç›®å½•
            output_dir = os.path.join(self.temp_dir, "output")
            os.makedirs(output_dir, exist_ok=True)
            
        
            self.logger.info("åˆ›å»ºæ–°çš„æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶")
            # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ ¼å¼ï¼‰
            test_case = {
                "case_1": {
                    "img_paths": [],
                    "audio_path": None,
                    "prompt": prompt
                }
            }
            
            # å¤„ç†å‚è€ƒå›¾åƒ
            if ref_image is not None and mode == "TIA":
                img_path = os.path.join(self.temp_dir, "ref_image.png")
                ref_image.save(img_path)
                test_case["case_1"]["img_paths"] = [img_path]
            elif mode == "TIA" and ref_image is None:
                self.is_generating = False
                return "", "âŒ TIAæ¨¡å¼éœ€è¦æä¾›å‚è€ƒå›¾åƒ", 0
            
            # å¤„ç†éŸ³é¢‘æ–‡ä»¶
            if audio_file is not None:
                self.logger.info(f"æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_file}")
                self.logger.info(f"éŸ³é¢‘æ–‡ä»¶ç±»å‹: {type(audio_file)}")
                
                # æ£€æŸ¥audio_fileæ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„
                if os.path.isfile(audio_file):
                    self.logger.info(f"éŸ³é¢‘æ–‡ä»¶éªŒè¯é€šè¿‡: {audio_file}")
                    audio_path = os.path.join(self.temp_dir, "audio.wav")
                    shutil.copy2(audio_file, audio_path)
                    test_case["case_1"]["audio_path"] = audio_path
                    self.logger.info(f"éŸ³é¢‘æ–‡ä»¶å·²å¤åˆ¶åˆ°: {audio_path}")
                elif os.path.isdir(audio_file):
                    self.logger.warning(f"éŸ³é¢‘æ–‡ä»¶è·¯å¾„æ˜¯ç›®å½•è€Œä¸æ˜¯æ–‡ä»¶: {audio_file}")
                    test_case["case_1"]["audio_path"] = None
                else:
                    self.logger.warning(f"éŸ³é¢‘æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–ä¸å­˜åœ¨: {audio_file}")
                    test_case["case_1"]["audio_path"] = None
            
            # ä¿å­˜æµ‹è¯•ç”¨ä¾‹
            test_case_path = os.path.join(self.temp_dir, "test_case.json")
            with open(test_case_path, 'w', encoding='utf-8') as f:
                json.dump(test_case, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"åˆ›å»ºçš„æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶: {test_case_path}")
            self.logger.info(f"æµ‹è¯•ç”¨ä¾‹å†…å®¹: {test_case}")
            
            # ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®æ›´æ–°æ–¹æ³•
            config_updates = {
                'mode': mode,
                'frames': frames,
                'height': height,
                'width': width,
                'scale_i': scale_i,
                'scale_a': scale_a,
                'scale_t': scale_t,
                'seed': seed,
                'fps': fps,
                'positive_prompt': test_case_path,
                'output': {'dir': output_dir}
            }
            
            # æ›´æ–°åå‘æç¤ºè¯é…ç½®
            if negative_prompt.strip():
                config_updates['sample_neg_prompt'] = negative_prompt
            
            # æ›´æ–°generationé…ç½®
            self.generator.update_generation_config(**config_updates)
            
            # æ›´æ–°å…¶ä»–é…ç½®ï¼ˆå¦‚diffusioné…ç½®ï¼‰
            self.generator.update_config(
                diffusion_timesteps_sampling_steps=sampling_steps
            )
            
            self.logger.info(f"é…ç½®æ›´æ–°å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆè§†é¢‘...")

            # è®°å½•ç”Ÿæˆå¼€å§‹æ—¶é—´
            generation_start_time = time.time()
            
            # ç”Ÿæˆè§†é¢‘ - ä½¿ç”¨inference_loopæ–¹æ³•
            self.generator.inference_loop()
            
            # è®°å½•ç”Ÿæˆç»“æŸæ—¶é—´å¹¶è®¡ç®—ç”Ÿæˆè€—æ—¶
            generation_end_time = time.time()
            generation_duration = generation_end_time - generation_start_time
            
            self.is_generating = False
            self.progress = 100  # è®¾ç½®ä¸º100%å®Œæˆ
            
            # inference_loopæ–¹æ³•ä¼šä¿å­˜è§†é¢‘æ–‡ä»¶ï¼ŒæŸ¥æ‰¾ç”Ÿæˆçš„æ–‡ä»¶
            output_files = list(Path(output_dir).glob("*.mp4"))
            
            self.logger.info(f"æŸ¥æ‰¾è¾“å‡ºç›®å½•: {output_dir}")
            self.logger.info(f"æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶: {output_files}")
            
            if output_files:
                video_path = str(output_files[0])
                # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                video_path = os.path.abspath(video_path)
                
                # è®¡ç®—æ€»è€—æ—¶
                total_duration = time.time() - start_time
                
                self.logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè§†é¢‘è·¯å¾„: {video_path}")
                self.logger.info(f"ç”Ÿæˆè€—æ—¶: {generation_duration:.2f}ç§’")
                self.logger.info(f"åˆæˆæ€»è€—æ—¶: {total_duration:.2f}ç§’")
                self.logger.debug(f"è§†é¢‘æ–‡ä»¶å­˜åœ¨: {os.path.exists(video_path)}")
                self.logger.debug(f"è§†é¢‘æ–‡ä»¶æ˜¯æ–‡ä»¶: {os.path.isfile(video_path)}")
                
                if os.path.exists(video_path):
                    success_message = f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼ç”Ÿæˆè€—æ—¶: {generation_duration:.2f}ç§’ï¼Œæ€»è€—æ—¶: {total_duration:.2f}ç§’"
                    return video_path, success_message, 100
                else:
                    self.logger.warning(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                    return "", "âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨", 0
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ï¼Œè¾“å‡ºç›®å½•å†…å®¹: {list(Path(output_dir).iterdir())}")
                return "", "âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶", 0
                
        except Exception as e:
            # è®¡ç®—æ€»è€—æ—¶ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼‰
            total_duration = time.time() - start_time
            self.logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œæ€»è€—æ—¶: {total_duration:.2f}ç§’")
            traceback.print_exc()
            self.is_generating = False
            return "", f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}", 0
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

# åˆ›å»ºåº”ç”¨å®ä¾‹
app = HuMoGradioApp()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="HuMo è§†é¢‘ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # ğŸ¬ HuMo å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆå™¨
        
        åŸºäºæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„äººç±»è§†é¢‘
        """)
        
        # æ¨¡å‹é…ç½®éƒ¨åˆ† - ç®€åŒ–å¸ƒå±€
        with gr.Row():
            with gr.Column(scale=4):
                model_type = gr.Dropdown(
                    choices=["1.7B", "17B"],
                    value="1.7B",
                    label="æ¨¡å‹ç±»å‹",
                    info="1.7B: è½»é‡çº§æ¨¡å‹ï¼Œé€Ÿåº¦å¿« | 17B: é«˜è´¨é‡æ¨¡å‹ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜"
                )
            with gr.Column(scale=1):
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
        
        model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
        
        # ä¸»è¦è¾“å…¥éƒ¨åˆ† - ä½¿ç”¨æ ‡ç­¾é¡µåŒºåˆ†åŸºæœ¬è®¾ç½®å’Œé«˜çº§å‚æ•°
        with gr.Tabs() as tabs:
            # åŸºæœ¬è®¾ç½®æ ‡ç­¾é¡µ - åªæ˜¾ç¤ºæ ¸å¿ƒå‚æ•°
            with gr.TabItem("åŸºæœ¬è®¾ç½®"):
                with gr.Row():
                    with gr.Column(scale=2):
                        # æ ¸å¿ƒå‚æ•°ï¼šæ–‡æœ¬æç¤º
                        prompt = gr.Textbox(
                            label="æ–‡æœ¬æç¤º (å¿…å¡«)",
                            placeholder="æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹...",
                            lines=3
                        )
                        
                        # åå‘æç¤ºè¯
                        negative_prompt = gr.Textbox(
                            label="åå‘æç¤ºè¯ (å¯é€‰)",
                            placeholder="æè¿°ä½ ä¸æƒ³è¦çš„å†…å®¹...",
                            lines=2,
                            value="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
                        )
                        
                        # æ ¸å¿ƒå‚æ•°ï¼šç”Ÿæˆæ¨¡å¼
                        mode = gr.Radio(
                            choices=["TA", "TIA"],
                            value="TA",  # é»˜è®¤ä½¿ç”¨TAæ¨¡å¼ï¼Œä¸éœ€è¦å‚è€ƒå›¾åƒ
                            label="ç”Ÿæˆæ¨¡å¼",
                            info="TA: æ–‡æœ¬+éŸ³é¢‘, TIA: æ–‡æœ¬+å›¾åƒ+éŸ³é¢‘"
                        )
                        
                        # æ ¸å¿ƒå‚æ•°ï¼šå‚è€ƒå›¾åƒå’ŒéŸ³é¢‘
                        with gr.Row():
                            with gr.Column(scale=1):
                                ref_image = gr.Image(
                                    label="å‚è€ƒå›¾åƒ (TIAæ¨¡å¼éœ€è¦)",
                                    type="pil",
                                    visible=False  # é»˜è®¤éšè—ï¼Œåªåœ¨TIAæ¨¡å¼ä¸‹æ˜¾ç¤º
                                )
                            
                            with gr.Column(scale=1):
                                audio_file = gr.Audio(
                                    label="éŸ³é¢‘æ–‡ä»¶",
                                    type="filepath"
                                )
                    
                    # ç¤ºä¾‹å’Œç”ŸæˆæŒ‰é’®
                    with gr.Column(scale=1):
                        # æ·»åŠ ä¸€äº›ç¤ºä¾‹æç¤º
                        gr.Markdown("### ç¤ºä¾‹æç¤º")
                        example_prompts = gr.Dataset(
                            components=[prompt],
                            samples=[
                                ["A person dancing energetically to upbeat music"],
                                ["Someone speaking confidently in a business meeting"],
                                ["A person playing guitar with passion"]
                            ],
                            label="ç‚¹å‡»åŠ è½½ç¤ºä¾‹æç¤º"
                        )
                        
                        # ç”ŸæˆæŒ‰é’®
                        generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
            
            # é«˜çº§å‚æ•°æ ‡ç­¾é¡µ - åŒ…å«æ‰€æœ‰å¯è°ƒæ•´çš„å‚æ•°
            with gr.TabItem("é«˜çº§å‚æ•°"):
                with gr.Row():
                    with gr.Column():
                        # è§†é¢‘å‚æ•°æŠ˜å é¢æ¿
                        with gr.Accordion("è§†é¢‘å‚æ•°", open=True):
                            with gr.Row():
                                frames = gr.Slider(
                                    minimum=-1, maximum=200, value=-1, step=1,
                                    label="è§†é¢‘å¸§æ•°",
                                    info="è®¾ç½®ä¸º-1æ—¶æ ¹æ®éŸ³é¢‘é•¿åº¦è‡ªåŠ¨è®¡ç®—å¸§æ•°ï¼Œå…¶ä»–å€¼æ‰‹åŠ¨æŒ‡å®šå¸§æ•°"
                                )
                                fps = gr.Slider(
                                    minimum=15, maximum=60, value=25, step=1,
                                    label="å¸§ç‡ (FPS)",
                                    info="æ¯ç§’æ˜¾ç¤ºçš„å¸§æ•°ï¼Œå½±å“è§†é¢‘æµç•…åº¦"
                                )
                            
                            resolution = gr.Dropdown(
                                choices=[
                                    "720P ç«–å± (720x1280)", 
                                    "720P æ¨ªå± (1280x720)",
                                    "480P ç«–å± (480x832)", 
                                    "480P æ¨ªå± (832x480)",
                                    "1024P æ–¹å½¢ (1024x1024)"
                                ],
                                value="720P ç«–å± (720x1280)",
                                label="è§†é¢‘åˆ†è¾¨ç‡",
                                info="720Pè´¨é‡æ›´å¥½ï¼Œ480Pé€Ÿåº¦æ›´å¿«ï¼›ç«–å±é€‚åˆæ‰‹æœºè§‚çœ‹ï¼Œæ¨ªå±é€‚åˆç”µè„‘è§‚çœ‹ï¼Œæ–¹å½¢é€‚åˆç¤¾äº¤åª’ä½“"
                            )
                        
                        # ç”Ÿæˆå‚æ•°æŠ˜å é¢æ¿
                        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=True):
                            with gr.Row():
                                scale_i = gr.Slider(
                                    minimum=1.0, maximum=10.0, value=5.0, step=0.1,
                                    label="å›¾åƒå¼•å¯¼å¼ºåº¦", 
                                    info="è¶Šé«˜è¶Šéµå¾ªå‚è€ƒå›¾åƒï¼Œæ¨èå€¼ï¼š4.0-6.0"
                                )
                                scale_a = gr.Slider(
                                    minimum=1.0, maximum=10.0, value=5.5, step=0.1,
                                    label="éŸ³é¢‘å¼•å¯¼å¼ºåº¦", 
                                    info="è¶Šé«˜éŸ³é¢‘åŒæ­¥è¶Šå¥½ï¼Œæ¨èå€¼ï¼š5.0-6.0"
                                )
                                scale_t = gr.Slider(
                                    minimum=1.0, maximum=10.0, value=5.0, step=0.1,
                                    label="æ–‡æœ¬å¼•å¯¼å¼ºåº¦", 
                                    info="è¶Šé«˜è¶Šéµå¾ªæ–‡æœ¬æç¤ºï¼Œæ¨èå€¼ï¼š4.0-6.0"
                                )
                            
                            with gr.Row():
                                sampling_steps = gr.Slider(
                                    minimum=20, maximum=100, value=50, step=5,
                                    label="é‡‡æ ·æ­¥æ•°", 
                                    info="æ›´å¤šæ­¥æ•°è´¨é‡æ›´å¥½ä½†æ›´æ…¢ï¼Œæ¨èå€¼ï¼š30-50"
                                )
                                seed = gr.Number(
                                    label="éšæœºç§å­",
                                    value=-1,
                                    precision=0,
                                    info="-1ä¸ºéšæœºç§å­ï¼Œå›ºå®šç§å­å¯é‡ç°ç»“æœ"
                                )
        
        # è¿›åº¦æ˜¾ç¤º
        progress_bar = gr.Progress()
        
        # è¾“å‡ºéƒ¨åˆ†
        with gr.Row():
            with gr.Column():
                output_video = gr.Video(
                    label="ç”Ÿæˆçš„è§†é¢‘",
                    height=400
                )
                status_text = gr.Textbox(
                    label="ç”ŸæˆçŠ¶æ€",
                    interactive=False,
                    lines=2
                )
        
        # äº‹ä»¶å¤„ç†
        def load_model_wrapper(model_type):
            return app.load_model(model_type)
        
        def generate_video_wrapper(prompt, negative_prompt, ref_image, audio_file, mode, frames, resolution, 
                                 scale_i, scale_a, scale_t, sampling_steps, seed, fps, progress=gr.Progress()):
            progress(0, desc="å‡†å¤‡ç”Ÿæˆ...")
            
            # è§£æåˆ†è¾¨ç‡
            if "720P æ¨ªå±" in resolution:
                height, width = 720, 1280
            elif "720P ç«–å±" in resolution:
                height, width = 1280, 720
            elif "480P æ¨ªå±" in resolution:
                height, width = 480, 832
            elif "480P ç«–å±" in resolution:
                height, width = 832, 480
            elif "1024P æ–¹å½¢" in resolution:
                height, width = 1024, 1024
            else:
                height, width = 720, 1280  # é»˜è®¤å€¼
            
            # åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°æ¥æ›´æ–°è¿›åº¦
            def progress_tracker():
                while app.is_generating:
                    progress_value = app.update_progress()
                    progress(progress_value/100)
                    time.sleep(0.5)
                    yield progress_value
            
            # å¯åŠ¨è¿›åº¦æ›´æ–°
            progress_gen = progress_tracker()
            
            # ç”Ÿæˆè§†é¢‘
            video_path, status, final_progress = app.generate_video(
                prompt, negative_prompt, ref_image, audio_file, mode, frames, height, width,
                scale_i, scale_a, scale_t, sampling_steps, seed, fps
            )
            
            # è®¾ç½®æœ€ç»ˆè¿›åº¦
            progress(final_progress/100)
            
            return video_path, status
        
        # æ¨¡å¼åˆ‡æ¢æ—¶æ˜¾ç¤º/éšè—å‚è€ƒå›¾åƒè¾“å…¥
        def update_ref_image_visibility(mode):
            return gr.update(visible=(mode == "TIA"))
        
        # å¤„ç†ç¤ºä¾‹æç¤ºé€‰æ‹©
        def load_example_prompt(evt: gr.SelectData):
            return evt.value[0]  # è¿”å›é€‰ä¸­çš„ç¤ºä¾‹æ–‡æœ¬
            
        # ç»‘å®šäº‹ä»¶
        load_btn.click(
            fn=load_model_wrapper,
            inputs=[model_type],
            outputs=[model_status]
        )
        
        mode.change(
            fn=update_ref_image_visibility,
            inputs=[mode],
            outputs=[ref_image]
        )

        # æ·»åŠ ç¤ºä¾‹æç¤ºé€‰æ‹©äº‹ä»¶
        example_prompts.select(
            fn=load_example_prompt,
            inputs=[],
            outputs=[prompt]
        )
        
        generate_btn.click(
            fn=generate_video_wrapper,
            inputs=[prompt, negative_prompt, ref_image, audio_file, mode, frames, resolution,
                   scale_i, scale_a, scale_t, sampling_steps, seed, fps],
            outputs=[output_video, status_text]
        )
        
        # æ·»åŠ ç¤ºä¾‹å’Œå¸®åŠ©ä¿¡æ¯
        with gr.Accordion("ğŸ’¡ ä½¿ç”¨å¸®åŠ©", open=False):
            gr.Markdown("""
            ## å¿«é€Ÿå…¥é—¨æŒ‡å—
            
            1. **é€‰æ‹©æ¨¡å‹**: ä»ä¸‹æ‹‰èœå•é€‰æ‹©æ¨¡å‹ç±»å‹
               - **1.7Bæ¨¡å‹**: è½»é‡çº§ï¼Œé€Ÿåº¦å¿«ï¼Œæ˜¾å­˜éœ€æ±‚ä½ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
               - **17Bæ¨¡å‹**: é«˜è´¨é‡ï¼Œæ•ˆæœå¥½ï¼Œæ˜¾å­˜éœ€æ±‚é«˜ï¼Œé€‚åˆé«˜è´¨é‡ç”Ÿæˆ
            2. **åŠ è½½æ¨¡å‹**: ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®åŠ è½½é€‰å®šçš„æ¨¡å‹
            3. **é€‰æ‹©æ¨¡å¼**: 
               - **TAæ¨¡å¼**: ä»…éœ€æ–‡æœ¬å’ŒéŸ³é¢‘ï¼ˆå¯é€‰ï¼‰
               - **TIAæ¨¡å¼**: éœ€è¦æ–‡æœ¬ã€å‚è€ƒå›¾åƒå’ŒéŸ³é¢‘ï¼ˆå¯é€‰ï¼‰
            4. **è¾“å…¥æç¤º**: è¯¦ç»†æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹
            5. **ä¸Šä¼ æ–‡ä»¶**: æ ¹æ®éœ€è¦ä¸Šä¼ å‚è€ƒå›¾åƒå’Œ/æˆ–éŸ³é¢‘æ–‡ä»¶
            6. **ç‚¹å‡»ç”Ÿæˆ**: ç­‰å¾…è§†é¢‘ç”Ÿæˆå®Œæˆ
            
            ## æç¤ºæŠ€å·§
            
            ### é«˜æ•ˆæ–‡æœ¬æç¤ºï¼š
            - **æ­£å‘æç¤ºè¯**ï¼šä½¿ç”¨å…·ä½“ã€è¯¦ç»†çš„æè¿°ï¼ˆå¦‚"A young woman with blonde hair dancing energetically to pop music"ï¼‰
            - **åå‘æç¤ºè¯**ï¼šæè¿°ä½ ä¸æƒ³è¦çš„å†…å®¹ï¼ˆå¦‚"æ¨¡ç³Šï¼Œä½è´¨é‡ï¼Œé™æ€ç”»é¢"ï¼‰
            - åŒ…å«åŠ¨ä½œã€è¡¨æƒ…å’Œåœºæ™¯ç»†èŠ‚ï¼ˆå¦‚"A man smiling and speaking confidently in a bright office setting"ï¼‰
            - æè¿°æƒ…æ„Ÿå’Œæ°›å›´ï¼ˆå¦‚"A person playing guitar passionately with eyes closed in a dimly lit room"ï¼‰
            
            ### å‚æ•°è°ƒæ•´å»ºè®®ï¼š
            - **éŸ³é¢‘å¼•å¯¼å¼ºåº¦ (scale_a)**: 5.0-6.0 é€‚åˆå¤§å¤šæ•°æƒ…å†µï¼Œæ›´é«˜å€¼ä½¿åŠ¨ä½œæ›´ç´§å¯†è·ŸéšéŸ³é¢‘
            - **æ–‡æœ¬å¼•å¯¼å¼ºåº¦ (scale_t)**: 4.0-6.0 å¹³è¡¡åˆ›æ„å’Œå‡†ç¡®æ€§ï¼Œæ›´é«˜å€¼ä½¿ç”Ÿæˆæ›´ç¬¦åˆæ–‡æœ¬æè¿°
            - **é‡‡æ ·æ­¥æ•°**: 30-50 å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ï¼Œæ›´å¤šæ­¥æ•°ç”Ÿæˆè´¨é‡æ›´é«˜ä½†è€—æ—¶æ›´é•¿
            - **è§†é¢‘åˆ†è¾¨ç‡**: 720P è´¨é‡æ›´å¥½ï¼Œ480P é€Ÿåº¦æ›´å¿«
            - **å¸§æ•°**: 97å¸§é€‚åˆå¤§å¤šæ•°åœºæ™¯ï¼Œå‡å°‘å¸§æ•°å¯åŠ å¿«ç”Ÿæˆé€Ÿåº¦
            
            ### å¸¸è§é—®é¢˜è§£ç­”ï¼š
            - **ç”Ÿæˆå¤±è´¥**: æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿TIAæ¨¡å¼ä¸‹æä¾›äº†å‚è€ƒå›¾åƒ
            - **è§†é¢‘è´¨é‡ä¸ä½³**: å°è¯•å¢åŠ é‡‡æ ·æ­¥æ•°ï¼Œè°ƒæ•´æ–‡æœ¬å’ŒéŸ³é¢‘å¼•å¯¼å¼ºåº¦
            - **åŠ¨ä½œä¸è‡ªç„¶**: è°ƒæ•´éŸ³é¢‘å¼•å¯¼å¼ºåº¦ï¼Œæˆ–ä½¿ç”¨æ›´æ¸…æ™°çš„éŸ³é¢‘æ–‡ä»¶
            - **ç”Ÿæˆç¼“æ…¢**: é™ä½åˆ†è¾¨ç‡ã€å‡å°‘å¸§æ•°æˆ–é‡‡æ ·æ­¥æ•°å¯åŠ å¿«ç”Ÿæˆé€Ÿåº¦
            
            ### æ³¨æ„äº‹é¡¹ï¼š
            - ç¡®ä¿æ¨¡å‹æ–‡ä»¶å·²æ­£ç¡®ä¸‹è½½åˆ°å¯¹åº”è·¯å¾„ï¼ˆ1.7B: ./weights/HuMo/HuMo-1.7B, 17B: ./weights/HuMo/HuMo-17Bï¼‰
            - 1.7Bæ¨¡å‹ä½¿ç”¨.pthæ ¼å¼ï¼Œ17Bæ¨¡å‹ä½¿ç”¨safetensorsæ ¼å¼
            - TIAæ¨¡å¼éœ€è¦æä¾›å‚è€ƒå›¾åƒ
            - éŸ³é¢‘æ–‡ä»¶æ”¯æŒWAVæ ¼å¼
            - ç”Ÿæˆæ—¶é—´å–å†³äºç¡¬ä»¶é…ç½®å’Œå‚æ•°è®¾ç½®
            - 17Bæ¨¡å‹éœ€è¦æ›´å¤šæ˜¾å­˜ï¼Œå»ºè®®è‡³å°‘520GBæ˜¾å­˜
            """)
    
    return interface

if __name__ == "__main__":
    # åˆ›å»ºç•Œé¢
    interface = create_interface()
    
    # å¯åŠ¨åº”ç”¨
    interface.launch(
        server_name="0.0.0.0",
        share=False,
        debug=True,
        show_error=True
    )