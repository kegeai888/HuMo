#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuMoè§†é¢‘ç”Ÿæˆå™¨ - å•é¡µé¢ä¼˜åŒ–ç‰ˆ
åŸºäºåŸgradio_app.pyçš„åŠŸèƒ½ï¼Œå•é¡µé¢è®¾è®¡ï¼Œç•Œé¢æ›´ç¾è§‚ç›´è§‚
"""

import gradio as gr
import os
import json
import tempfile
import shutil
from pathlib import Path
import torch
import sys
from typing import Optional, Tuple, List
import numpy as np
from PIL import Image
import mediapy
import time
import traceback
import gc

# æ·»åŠ é¡¹ç›®è·¯å¾„
path_to_insert = "humo"
if path_to_insert not in sys.path:
    sys.path.insert(0, path_to_insert)

from common.config import load_config, create_object
from common.distributed import get_device, get_global_rank, init_torch
from common.logger import get_logger
from humo.models.utils.utils import tensor_to_video

# è§†é¢‘å°ºå¯¸é…ç½®
SIZE_CONFIGS = {
    '720*1280': (720, 1280),
    '1280*720': (1280, 720),
    '480*832': (480, 832),
    '832*480': (832, 480),
    '1024*1024': (1024, 1024),
}

class HuMoGradioApp:
    def __init__(self):
        self.generator = None
        self.config = None
        self.temp_dir = tempfile.mkdtemp()
        self.progress = 0
        self.is_generating = False
        self.model_type = None  # è®°å½•å½“å‰æ¨¡å‹ç±»å‹
        self.logger = get_logger(self.__class__.__name__)

        # GPUä¼˜åŒ–è®¾ç½®
        self._setup_gpu_optimization()

    def _setup_gpu_optimization(self):
        """è®¾ç½®GPUä¼˜åŒ–"""
        if torch.cuda.is_available():
            # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
            torch.cuda.set_per_process_memory_fraction(0.95)

            # å¯ç”¨CuDNNåŸºå‡†æµ‹è¯•ä»¥ä¼˜åŒ–æ€§èƒ½
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False

            # è®¾ç½®CUDAå†…å­˜åˆ†é…å™¨
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

            self.logger.info(f"GPUä¼˜åŒ–å·²å¯ç”¨ï¼Œè®¾å¤‡: {torch.cuda.get_device_name()}")
            self.logger.info(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            self.logger.info(f"æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            self.logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")

    def get_gpu_status(self) -> dict:
        """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
        if not torch.cuda.is_available():
            return {
                "available": False,
                "message": "âŒ CUDAä¸å¯ç”¨",
                "details": "è¯·æ£€æŸ¥CUDAå®‰è£…å’ŒGPUé©±åŠ¨"
            }

        try:
            device = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(device)

            total_memory = props.total_memory
            allocated_memory = torch.cuda.memory_allocated(device)
            reserved_memory = torch.cuda.memory_reserved(device)
            free_memory = total_memory - allocated_memory

            return {
                "available": True,
                "device_name": props.name,
                "device_count": torch.cuda.device_count(),
                "current_device": device,
                "total_memory_gb": total_memory / 1024**3,
                "allocated_memory_gb": allocated_memory / 1024**3,
                "reserved_memory_gb": reserved_memory / 1024**3,
                "free_memory_gb": free_memory / 1024**3,
                "utilization": (allocated_memory / total_memory) * 100,
                "cuda_version": torch.version.cuda,
                "message": f"âœ… GPUå¯ç”¨: {props.name}"
            }
        except Exception as e:
            return {
                "available": False,
                "message": f"âŒ GPUçŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}",
                "details": str(e)
            }

    def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        try:
            # Pythonåƒåœ¾å›æ”¶
            gc.collect()

            # æ¸…ç†CUDAç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
                # åªæ¸…ç†æ—§æ–‡ä»¶ï¼Œä¿ç•™å½“å‰ä¼šè¯çš„æ–‡ä»¶
                for root, dirs, files in os.walk(self.temp_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        try:
                            # åˆ é™¤è¶…è¿‡1å°æ—¶çš„ä¸´æ—¶æ–‡ä»¶
                            if time.time() - os.path.getctime(file_path) > 3600:
                                os.remove(file_path)
                        except:
                            pass

            self.logger.info("å†…å­˜ä¼˜åŒ–å®Œæˆ")
            return "âœ… å†…å­˜ä¼˜åŒ–å®Œæˆ"
        except Exception as e:
            self.logger.error(f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {str(e)}")
            return f"âŒ å†…å­˜ä¼˜åŒ–å¤±è´¥: {str(e)}"

    def load_model(self, model_type: str = "1.7B"):
        """åŠ è½½HuMoæ¨¡å‹"""
        try:
            # å…ˆè¿›è¡Œå†…å­˜ä¼˜åŒ–
            self.optimize_memory()

            # å…ˆé‡Šæ”¾æ˜¾å­˜
            gc.collect()
            torch.cuda.empty_cache()

            # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®è·¯å¾„å’Œé…ç½®
            if model_type == "1.7B":
                model_path = "./weights/HuMo/HuMo-1.7B"
                config_path = "humo/configs/inference/generate_1_7B.yaml"
                self.model_type = "1.7B"
            elif model_type == "17B":
                model_path = "./weights/HuMo/HuMo-17B"
                config_path = "humo/configs/inference/generate.yaml"
                self.model_type = "17B"
            else:
                return f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}"

            if not os.path.exists(model_path):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"

            if not os.path.exists(config_path):
                return f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}"

            # æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ
            gpu_status = self.get_gpu_status()
            if gpu_status["available"]:
                required_memory = 8.0 if model_type == "1.7B" else 24.0
                if gpu_status["free_memory_gb"] < required_memory:
                    return f"âŒ GPUæ˜¾å­˜ä¸è¶³ï¼Œéœ€è¦ {required_memory}GBï¼Œå¯ç”¨ {gpu_status['free_memory_gb']:.1f}GB"

            # åŠ è½½é…ç½®
            self.config = load_config(config_path, [])
            self.config.dit.checkpoint_dir = model_path

            # åœ¨å•æœºæ¨¡å¼ä¸‹ç¦ç”¨åºåˆ—å¹¶è¡Œ
            import torch.distributed as dist
            if not dist.is_initialized() or dist.get_world_size() == 1:
                self.config.dit.sp_size = 1
                self.config.generation.sequence_parallel = 1
                print("ğŸ”§ å•æœºæ¨¡å¼ï¼šå·²ç¦ç”¨åºåˆ—å¹¶è¡Œ")

            # åˆå§‹åŒ–torch
            init_torch(cudnn_benchmark=False)

            # åˆ›å»ºç”Ÿæˆå™¨
            self.generator = create_object(self.config)

            # é…ç½®æ¨¡å‹ç»„ä»¶ï¼ˆåŒ…æ‹¬vaeï¼‰
            self.generator.configure_models()

            # å†æ¬¡æ£€æŸ¥å†…å­˜çŠ¶æ€
            gpu_status = self.get_gpu_status()
            memory_info = ""
            if gpu_status["available"]:
                memory_info = f"\nğŸ–¥ï¸ GPU: {gpu_status['device_name']}\nğŸ“Š æ˜¾å­˜ä½¿ç”¨: {gpu_status['allocated_memory_gb']:.1f}GB / {gpu_status['total_memory_gb']:.1f}GB"

            return f"âœ… {model_type}æ¨¡å‹åŠ è½½æˆåŠŸï¼{memory_info}"
        except Exception as e:
            traceback.print_exc()
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

    def update_progress(self) -> float:
        """æ›´æ–°è¿›åº¦æ¡"""
        if not self.is_generating:
            return 0
        self.progress += 1
        # æ¨¡æ‹Ÿè¿›åº¦ï¼Œæœ€å¤§å€¼ä¸º99%ï¼Œç•™1%ç»™æœ€ç»ˆå¤„ç†
        progress_value = min(99, self.progress)
        return progress_value

    def generate_video(
        self,
        prompt: str,
        negative_prompt: str = "",
        ref_image: Optional[Image.Image] = None,
        audio_file: Optional[str] = None,
        mode: str = "TA",
        frames: int = 97,
        height: int = 720,
        width: int = 1280,
        scale_i: float = 5.0,
        scale_a: float = 5.5,
        scale_t: float = 5.0,
        sampling_steps: int = 50,
        seed: int = 666666,
        fps: int = 25
    ) -> Tuple[str, str, float]:
        """ç”Ÿæˆè§†é¢‘"""
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        try:
            # é‡ç½®è¿›åº¦
            self.progress = 0
            self.is_generating = True

            # è¯¦ç»†çš„è¾“å…¥éªŒè¯
            if self.generator is None:
                self.is_generating = False
                return "", "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹\nğŸ’¡ æç¤ºï¼šåœ¨æ¨¡å‹é…ç½®åŒºåŸŸé€‰æ‹©æ¨¡å‹ç±»å‹å¹¶ç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®", 0

            if not prompt.strip():
                self.is_generating = False
                return "", "âŒ è¯·è¾“å…¥æ–‡æœ¬æç¤ºè¯\nğŸ’¡ æç¤ºï¼šè¯¦ç»†æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹ï¼Œä¾‹å¦‚'ä¸€ä½å¹´è½»å¥³æ€§åœ¨éŸ³ä¹èŠ‚ä¸Šçƒ­æƒ…åœ°è·³èˆ'", 0

            if mode == "TIA" and ref_image is None:
                self.is_generating = False
                return "", "âŒ TIAæ¨¡å¼éœ€è¦æä¾›å‚è€ƒå›¾åƒ\nğŸ’¡ æç¤ºï¼šè¯·ä¸Šä¼ ä¸€å¼ å‚è€ƒå›¾åƒï¼Œæˆ–åˆ‡æ¢åˆ°TAæ¨¡å¼", 0

            # GPUå†…å­˜æ£€æŸ¥
            gpu_status = self.get_gpu_status()
            if gpu_status["available"]:
                if gpu_status["free_memory_gb"] < 4.0:
                    self.is_generating = False
                    return "", f"âŒ GPUæ˜¾å­˜ä¸è¶³ ({gpu_status['free_memory_gb']:.1f}GBå¯ç”¨)\nğŸ’¡ æç¤ºï¼šç‚¹å‡»'ä¼˜åŒ–å†…å­˜'æŒ‰é’®é‡Šæ”¾æ˜¾å­˜ï¼Œæˆ–é™ä½åˆ†è¾¨ç‡/å¸§æ•°", 0

            # å‚æ•°åˆç†æ€§æ£€æŸ¥
            if frames > 200:
                return "", "âŒ å¸§æ•°è¿‡å¤šï¼Œå»ºè®®ä¸è¶…è¿‡200å¸§\nğŸ’¡ æç¤ºï¼šè¿‡å¤šå¸§æ•°ä¼šæ˜¾è‘—å¢åŠ ç”Ÿæˆæ—¶é—´å’Œæ˜¾å­˜å ç”¨", 0

            if sampling_steps > 100:
                return "", "âŒ é‡‡æ ·æ­¥æ•°è¿‡å¤šï¼Œå»ºè®®ä¸è¶…è¿‡100æ­¥\nğŸ’¡ æç¤ºï¼šè¿‡å¤šé‡‡æ ·æ­¥æ•°ä¼šæ˜¾è‘—å¢åŠ ç”Ÿæˆæ—¶é—´ï¼Œé€šå¸¸30-50æ­¥å·²è¶³å¤Ÿ", 0

            # å‡†å¤‡è¾“å‡ºç›®å½•
            output_dir = os.path.join(self.temp_dir, "output")
            os.makedirs(output_dir, exist_ok=True)

            self.logger.info("å¼€å§‹åˆ›å»ºæµ‹è¯•ç”¨ä¾‹æ–‡ä»¶")
            # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ ¼å¼ï¼‰
            test_case = {
                "case_1": {
                    "img_paths": [],
                    "audio_path": None,
                    "prompt": prompt
                }
            }

            # å¤„ç†å‚è€ƒå›¾åƒ
            if ref_image is not None and mode == "TIA":
                try:
                    img_path = os.path.join(self.temp_dir, "ref_image.png")
                    ref_image.save(img_path)
                    test_case["case_1"]["img_paths"] = [img_path]
                    self.logger.info(f"å‚è€ƒå›¾åƒå·²ä¿å­˜: {img_path}")
                except Exception as e:
                    self.is_generating = False
                    return "", f"âŒ å‚è€ƒå›¾åƒå¤„ç†å¤±è´¥: {str(e)}\nğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥å›¾åƒæ ¼å¼æ˜¯å¦æ­£ç¡®", 0

            # å¤„ç†éŸ³é¢‘æ–‡ä»¶
            if audio_file is not None:
                self.logger.info(f"æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_file}")

                try:
                    if os.path.isfile(audio_file):
                        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶å¤§å°
                        file_size = os.path.getsize(audio_file) / (1024 * 1024)  # MB
                        if file_size > 100:  # 100MBé™åˆ¶
                            self.is_generating = False
                            return "", f"âŒ éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ ({file_size:.1f}MB)\nğŸ’¡ æç¤ºï¼šè¯·ä½¿ç”¨å°äº100MBçš„éŸ³é¢‘æ–‡ä»¶", 0

                        audio_path = os.path.join(self.temp_dir, "audio.wav")
                        shutil.copy2(audio_file, audio_path)
                        test_case["case_1"]["audio_path"] = audio_path
                        self.logger.info(f"éŸ³é¢‘æ–‡ä»¶å·²å¤åˆ¶åˆ°: {audio_path}")
                    else:
                        self.logger.warning(f"éŸ³é¢‘æ–‡ä»¶è·¯å¾„æ— æ•ˆ: {audio_file}")
                        test_case["case_1"]["audio_path"] = None
                except Exception as e:
                    self.logger.warning(f"éŸ³é¢‘æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
                    test_case["case_1"]["audio_path"] = None

            # ä¿å­˜æµ‹è¯•ç”¨ä¾‹
            test_case_path = os.path.join(self.temp_dir, "test_case.json")
            with open(test_case_path, 'w', encoding='utf-8') as f:
                json.dump(test_case, f, ensure_ascii=False, indent=2)

            self.logger.info(f"æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶å·²åˆ›å»º: {test_case_path}")

            # ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®æ›´æ–°æ–¹æ³•
            config_updates = {
                'mode': mode,
                'frames': frames,
                'height': height,
                'width': width,
                'scale_i': scale_i,
                'scale_a': scale_a,
                'scale_t': scale_t,
                'seed': seed,
                'fps': fps,
                'positive_prompt': test_case_path,
                'output': {'dir': output_dir}
            }

            # æ›´æ–°åå‘æç¤ºè¯é…ç½®
            if negative_prompt.strip():
                config_updates['sample_neg_prompt'] = negative_prompt

            # æ›´æ–°generationé…ç½®
            self.generator.update_generation_config(**config_updates)

            # æ›´æ–°å…¶ä»–é…ç½®ï¼ˆå¦‚diffusioné…ç½®ï¼‰
            self.generator.update_config(
                diffusion_timesteps_sampling_steps=sampling_steps
            )

            self.logger.info("é…ç½®æ›´æ–°å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆè§†é¢‘...")

            # è®°å½•ç”Ÿæˆå¼€å§‹æ—¶é—´
            generation_start_time = time.time()

            # ç”Ÿæˆè§†é¢‘ - ä½¿ç”¨inference_loopæ–¹æ³•
            self.generator.inference_loop()

            # è®°å½•ç”Ÿæˆç»“æŸæ—¶é—´å¹¶è®¡ç®—ç”Ÿæˆè€—æ—¶
            generation_end_time = time.time()
            generation_duration = generation_end_time - generation_start_time

            self.is_generating = False
            self.progress = 100  # è®¾ç½®ä¸º100%å®Œæˆ

            # inference_loopæ–¹æ³•ä¼šä¿å­˜è§†é¢‘æ–‡ä»¶ï¼ŒæŸ¥æ‰¾ç”Ÿæˆçš„æ–‡ä»¶
            output_files = list(Path(output_dir).glob("*.mp4"))

            self.logger.info(f"æŸ¥æ‰¾è¾“å‡ºç›®å½•: {output_dir}")
            self.logger.info(f"æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶: {output_files}")

            if output_files:
                video_path = str(output_files[0])
                # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                video_path = os.path.abspath(video_path)

                # è®¡ç®—æ€»è€—æ—¶
                total_duration = time.time() - start_time

                self.logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè§†é¢‘è·¯å¾„: {video_path}")

                if os.path.exists(video_path):
                    # è·å–è§†é¢‘æ–‡ä»¶å¤§å°
                    file_size = os.path.getsize(video_path) / (1024 * 1024)  # MB

                    success_message = (
                        f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼\n"
                        f"â±ï¸ ç”Ÿæˆè€—æ—¶: {generation_duration:.1f}ç§’\n"
                        f"ğŸ“Š æ€»è€—æ—¶: {total_duration:.1f}ç§’\n"
                        f"ğŸ¬ è§†é¢‘å¤§å°: {file_size:.1f}MB\n"
                        f"ğŸ“ åˆ†è¾¨ç‡: {width}x{height}\n"
                        f"ğŸï¸ å¸§æ•°: {frames} | å¸§ç‡: {fps}FPS\n"
                        f"ğŸ¤– æ¨¡å‹: {self.model_type}"
                    )
                    return video_path, success_message, 100
                else:
                    self.logger.warning(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                    return "", "âŒ è§†é¢‘æ–‡ä»¶ç”Ÿæˆåä¸¢å¤±\nğŸ’¡ æç¤ºï¼šå¯èƒ½æ˜¯ç£ç›˜ç©ºé—´ä¸è¶³æˆ–æƒé™é—®é¢˜", 0
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ï¼Œè¾“å‡ºç›®å½•å†…å®¹: {list(Path(output_dir).iterdir())}")
                return "", "âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶\nğŸ’¡ æç¤ºï¼šç”Ÿæˆå¯èƒ½æœªå®Œæˆæˆ–ä¿å­˜å¤±è´¥ï¼Œè¯·é‡è¯•", 0

        except torch.cuda.OutOfMemoryError as e:
            total_duration = time.time() - start_time
            self.logger.error(f"GPUæ˜¾å­˜ä¸è¶³ï¼Œæ€»è€—æ—¶: {total_duration:.1f}ç§’")
            self.is_generating = False
            return "", "âŒ GPUæ˜¾å­˜ä¸è¶³\nğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š\n1. ç‚¹å‡»'ä¼˜åŒ–å†…å­˜'æŒ‰é’®\n2. é™ä½åˆ†è¾¨ç‡æˆ–å¸§æ•°\n3. ä½¿ç”¨1.7Bæ¨¡å‹è€Œé17Bæ¨¡å‹", 0

        except FileNotFoundError as e:
            total_duration = time.time() - start_time
            self.logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ€»è€—æ—¶: {total_duration:.1f}ç§’")
            self.is_generating = False
            return "", f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}\nğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦æ­£ç¡®ä¸‹è½½", 0

        except PermissionError as e:
            total_duration = time.time() - start_time
            self.logger.error(f"æƒé™é”™è¯¯ï¼Œæ€»è€—æ—¶: {total_duration:.1f}ç§’")
            self.is_generating = False
            return "", f"âŒ æƒé™é”™è¯¯\nğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥æ–‡ä»¶å¤¹æƒé™æˆ–ä½¿ç”¨ç®¡ç†å‘˜æƒé™è¿è¡Œ", 0

        except Exception as e:
            # è®¡ç®—æ€»è€—æ—¶ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼‰
            total_duration = time.time() - start_time
            self.logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œæ€»è€—æ—¶: {total_duration:.1f}ç§’")
            traceback.print_exc()
            self.is_generating = False

            # æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
            error_message = f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}\nğŸ’¡ å¸¸è§è§£å†³æ–¹æ¡ˆï¼š\n"
            if "CUDA" in str(e):
                error_message += "â€¢ é‡å¯åº”ç”¨å¹¶é‡æ–°åŠ è½½æ¨¡å‹\nâ€¢ æ£€æŸ¥CUDAé©±åŠ¨å’ŒPyTorchç‰ˆæœ¬"
            elif "memory" in str(e).lower():
                error_message += "â€¢ é™ä½åˆ†è¾¨ç‡ã€å¸§æ•°æˆ–é‡‡æ ·æ­¥æ•°\nâ€¢ ç‚¹å‡»'ä¼˜åŒ–å†…å­˜'æŒ‰é’®"
            elif "model" in str(e).lower():
                error_message += "â€¢ é‡æ–°ä¸‹è½½æ¨¡å‹æ–‡ä»¶\nâ€¢ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"
            else:
                error_message += "â€¢ æ£€æŸ¥è¾“å…¥å‚æ•°æ˜¯å¦åˆç†\nâ€¢ é‡å¯åº”ç”¨é‡è¯•"

            return "", error_message, 0

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

# åˆ›å»ºåº”ç”¨å®ä¾‹
app = HuMoGradioApp()

def create_interface():
    """åˆ›å»ºå•é¡µé¢Gradioç•Œé¢"""

    # ç²¾ç¾çš„CSSæ ·å¼
    custom_css = """
    .gradio-container {
        font-family: 'Inter', 'Microsoft YaHei', sans-serif;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        min-height: 100vh;
    }

    .main-container {
        background: white;
        border-radius: 20px;
        padding: 25px;
        margin: 20px;
        box-shadow: 0 20px 40px rgba(0,0,0,0.1);
    }

    .header-section {
        text-align: center;
        padding: 30px 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 20px;
        margin-bottom: 30px;
        color: white;
    }

    .section-title {
        font-size: 1.3em;
        font-weight: 600;
        color: #374151;
        margin: 20px 0 15px 0;
        padding-bottom: 8px;
        border-bottom: 2px solid #e5e7eb;
    }

    .gr-button {
        border-radius: 12px;
        font-weight: 600;
        transition: all 0.3s ease;
        font-size: 1em;
    }

    .gr-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(0,0,0,0.15);
    }

    .gr-form {
        border-radius: 15px;
        border: 1px solid #e5e7eb;
        background: #fafafa;
    }

    .parameter-box {
        background: #f8fafc;
        border-radius: 12px;
        padding: 20px;
        border: 1px solid #e2e8f0;
        margin: 10px 0;
    }

    .status-box {
        background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
        border-radius: 12px;
        padding: 15px;
        border-left: 4px solid #0ea5e9;
        margin: 10px 0;
    }

    .video-output {
        border-radius: 15px;
        overflow: hidden;
        box-shadow: 0 10px 30px rgba(0,0,0,0.1);
    }

    .example-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 10px;
        margin: 15px 0;
    }

    .example-item {
        background: #f1f5f9;
        padding: 12px;
        border-radius: 8px;
        cursor: pointer;
        transition: all 0.2s ease;
        border: 1px solid #e2e8f0;
    }

    .example-item:hover {
        background: #e2e8f0;
        transform: scale(1.02);
    }
    """

    with gr.Blocks(
        title="HuMo è§†é¢‘ç”Ÿæˆå™¨",
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="purple",
            neutral_hue="gray",
            font=gr.themes.GoogleFont("Inter")
        ),
        css=custom_css
    ) as interface:

        # ä¸»å®¹å™¨
        with gr.Column(elem_classes="main-container"):

            # ç²¾ç¾çš„å¤´éƒ¨
            gr.HTML("""
            <div class="header-section">
                <h1 style="font-size: 3.5em; margin-bottom: 15px; font-weight: 700;">ğŸ¬ HuMo è§†é¢‘ç”Ÿæˆå™¨</h1>
                <p style="font-size: 1.3em; opacity: 0.95; margin: 5px 0;">åŸºäºåä½œå¤šæ¨¡æ€æ¡ä»¶çš„äººä½“ä¸­å¿ƒè§†é¢‘ç”Ÿæˆ</p>
                <p style="font-size: 1.1em; opacity: 0.85; margin-top: 15px;">âœ¨ æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å¤šç§è¾“å…¥æ¨¡å¼çš„æ™ºèƒ½è§†é¢‘ç”Ÿæˆ</p>
                <p style="font-size: 1.1em; opacity: 0.85; margin-top: 15px; color: white;">
    âœ¨ äºŒæ¬¡å¼€å‘æ„å»º Byï¼šç§‘å“¥ 
    <a href="https://github.com/kegeai888/HuMo" style="color: white;">æ¬¢è¿ start</a>
</p>

            </div>
            """)

            # æ¨¡å‹é…ç½®åŒºåŸŸ
            gr.HTML('<div class="section-title">ğŸ¤– æ¨¡å‹é…ç½®</div>')
            with gr.Row():
                with gr.Column(scale=3):
                    model_type = gr.Dropdown(
                        choices=["1.7B", "17B"],
                        value="1.7B",
                        label="é€‰æ‹©æ¨¡å‹ç±»å‹",
                        info="ğŸ’¡ 1.7B: è½»é‡çº§ï¼Œé€Ÿåº¦å¿«ï¼Œæ˜¾å­˜éœ€æ±‚ä½ | 17B: é«˜è´¨é‡ï¼Œæ•ˆæœä½³ï¼Œæ˜¾å­˜éœ€æ±‚é«˜",
                        elem_classes="parameter-box"
                    )
                with gr.Column(scale=1):
                    load_btn = gr.Button(
                        "ğŸ”„ åŠ è½½æ¨¡å‹",
                        variant="primary",
                        size="lg"
                    )

            model_status = gr.Textbox(
                label="ğŸ“Š æ¨¡å‹çŠ¶æ€",
                interactive=False,
                elem_classes="status-box"
            )

            # GPUçŠ¶æ€ç›‘æ§
            with gr.Row():
                with gr.Column(scale=2):
                    gpu_status_display = gr.Textbox(
                        label="ğŸ–¥ï¸ GPUçŠ¶æ€",
                        interactive=False,
                        lines=2,
                        elem_classes="status-box"
                    )
                with gr.Column(scale=1):
                    with gr.Row():
                        gpu_info_btn = gr.Button("ğŸ” æ£€æŸ¥GPU", size="sm")
                        memory_optimize_btn = gr.Button("ğŸ§¹ ä¼˜åŒ–å†…å­˜", size="sm")

            # ä¸»è¦è¾“å…¥åŒºåŸŸ
            gr.HTML('<div class="section-title">âœ¨ å†…å®¹åˆ›ä½œ</div>')
            with gr.Row():
                with gr.Column(scale=2):
                    # æ–‡æœ¬è¾“å…¥
                    prompt = gr.Textbox(
                        label="ğŸ¯ æ–‡æœ¬æç¤ºè¯ (å¿…å¡«)",
                        placeholder="è¯¦ç»†æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹ï¼Œä¾‹å¦‚ï¼šä¸€ä½å¹´è½»å¥³æ€§åœ¨éŸ³ä¹èŠ‚ä¸Šçƒ­æƒ…åœ°è·³èˆï¼Œä¼´éšç€èŠ‚æ‹æŒ¥èˆåŒæ‰‹...",
                        lines=4,
                        max_lines=8
                    )

                    negative_prompt = gr.Textbox(
                        label="ğŸš« è´Ÿé¢æç¤ºè¯ (å¯é€‰)",
                        placeholder="æè¿°ä½ ä¸æƒ³è¦çš„å†…å®¹...",
                        lines=2,
                        value="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
                    )

                    # ç”Ÿæˆæ¨¡å¼
                    mode = gr.Radio(
                        choices=[("æ–‡æœ¬+éŸ³é¢‘", "TA"), ("æ–‡æœ¬+å›¾åƒ+éŸ³é¢‘", "TIA")],
                        value="TA",
                        label="ğŸ­ ç”Ÿæˆæ¨¡å¼",
                        info="é€‰æ‹©è¾“å…¥æ¨¡å¼ï¼šTAæ¨¡å¼æ— éœ€å‚è€ƒå›¾åƒï¼ŒTIAæ¨¡å¼å¯æ§åˆ¶äººç‰©å¤–è§‚"
                    )

                with gr.Column(scale=1):
                    # æ–‡ä»¶ä¸Šä¼ 
                    ref_image = gr.Image(
                        label="ğŸ–¼ï¸ å‚è€ƒå›¾åƒ (TIAæ¨¡å¼éœ€è¦)",
                        type="pil",
                        visible=False
                    )

                    audio_file = gr.Audio(
                        label="ğŸµ éŸ³é¢‘æ–‡ä»¶ (å¯é€‰)",
                        type="filepath"
                    )

                    # ç¤ºä¾‹æç¤º
                    gr.Markdown("### ğŸ’¡ ç¤ºä¾‹æç¤º")
                    example_prompts = gr.Dataset(
                        components=[prompt],
                        samples=[
                            ["ä¸€ä½å¹´è½»å¥³æ€§åœ¨éŸ³ä¹èŠ‚ä¸Šçƒ­æƒ…åœ°è·³èˆï¼Œä¼´éšç€èŠ‚æ‹æŒ¥èˆåŒæ‰‹ï¼Œè„¸ä¸Šæ´‹æº¢ç€å¿«ä¹çš„ç¬‘å®¹"],
                            ["ä¸€ä¸ªç”·äººåœ¨åŠå…¬å®¤é‡Œè‡ªä¿¡åœ°è¿›è¡Œå•†åŠ¡æ¼”è®²ï¼Œæ‰‹åŠ¿è‡ªç„¶æµç•…ï¼Œè¡¨æƒ…ä¸“ä¸šè®¤çœŸ"],
                            ["ä¸€ä½éŸ³ä¹å®¶åœ¨æ˜æš—çš„æˆ¿é—´é‡Œå……æ»¡æ¿€æƒ…åœ°å¼¹å¥å‰ä»–ï¼Œçœ¼ç¥ä¸“æ³¨ï¼Œå®Œå…¨æ²‰æµ¸åœ¨éŸ³ä¹ä¸­"],
                            ["ä¸€ä¸ªå­©å­åœ¨èŠ±å›­é‡Œå¿«ä¹åœ°å¥”è·‘ç©è€ï¼Œé˜³å…‰æ´’åœ¨è„¸ä¸Šï¼ŒåŠ¨ä½œæ´»æ³¼è‡ªç„¶"]
                        ],
                        label="ç‚¹å‡»åŠ è½½ç¤ºä¾‹",
                        elem_classes="example-grid"
                    )

            # å‚æ•°é…ç½®åŒºåŸŸ
            gr.HTML('<div class="section-title">âš™ï¸ å‚æ•°é…ç½®</div>')
            with gr.Row():
                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### ğŸ“ è§†é¢‘å‚æ•°")
                        with gr.Row():
                            frames = gr.Slider(
                                minimum=-1, maximum=200, value=-1, step=1,
                                label="è§†é¢‘å¸§æ•°",
                                info="-1 è¡¨ç¤ºæ ¹æ®éŸ³é¢‘é•¿åº¦è‡ªåŠ¨è®¡ç®—"
                            )
                            fps = gr.Slider(
                                minimum=15, maximum=60, value=25, step=1,
                                label="å¸§ç‡ (FPS)"
                            )

                        resolution = gr.Dropdown(
                            choices=[
                                "720P ç«–å± (720x1280)",
                                "720P æ¨ªå± (1280x720)",
                                "480P ç«–å± (480x832)",
                                "480P æ¨ªå± (832x480)",
                                "1024P æ–¹å½¢ (1024x1024)"
                            ],
                            value="720P ç«–å± (720x1280)",
                            label="è§†é¢‘åˆ†è¾¨ç‡",
                            info="720P è´¨é‡æ›´å¥½ï¼Œ480P é€Ÿåº¦æ›´å¿«"
                        )

                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### ğŸ›ï¸ ç”Ÿæˆå‚æ•°")
                        with gr.Row():
                            scale_t = gr.Slider(
                                minimum=1.0, maximum=15.0, value=5.0, step=0.1,
                                label="æ–‡æœ¬å¼•å¯¼å¼ºåº¦",
                                info="æ§åˆ¶å¯¹æ–‡æœ¬æè¿°çš„éµå¾ªç¨‹åº¦"
                            )
                            scale_a = gr.Slider(
                                minimum=1.0, maximum=15.0, value=5.5, step=0.1,
                                label="éŸ³é¢‘å¼•å¯¼å¼ºåº¦",
                                info="æ§åˆ¶åŠ¨ä½œä¸éŸ³é¢‘çš„åŒæ­¥ç¨‹åº¦"
                            )
                            scale_i = gr.Slider(
                                minimum=1.0, maximum=15.0, value=5.0, step=0.1,
                                label="å›¾åƒå¼•å¯¼å¼ºåº¦",
                                info="æ§åˆ¶å¯¹å‚è€ƒå›¾åƒçš„ç›¸ä¼¼ç¨‹åº¦"
                            )

                        with gr.Row():
                            sampling_steps = gr.Slider(
                                minimum=20, maximum=100, value=50, step=5,
                                label="é‡‡æ ·æ­¥æ•°",
                                info="æ›´å¤šæ­¥æ•°è´¨é‡æ›´å¥½ä½†æ—¶é—´æ›´é•¿"
                            )
                            seed = gr.Number(
                                label="éšæœºç§å­",
                                value=-1,
                                precision=0,
                                info="-1 ä¸ºéšæœºç§å­"
                            )

            # ç”ŸæˆæŒ‰é’®
            generate_btn = gr.Button(
                "ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘",
                variant="primary",
                size="lg",
                elem_id="generate-btn"
            )

            # è¾“å‡ºåŒºåŸŸ
            gr.HTML('<div class="section-title">ğŸ¥ ç”Ÿæˆç»“æœ</div>')
            with gr.Row():
                with gr.Column():
                    output_video = gr.Video(
                        label="ç”Ÿæˆçš„è§†é¢‘",
                        height=450,
                        elem_classes="video-output"
                    )
                    status_text = gr.Textbox(
                        label="ğŸ“Š ç”ŸæˆçŠ¶æ€",
                        interactive=False,
                        lines=3,
                        elem_classes="status-box"
                    )

            # ä½¿ç”¨å¸®åŠ©
            with gr.Accordion("ğŸ’¡ ä½¿ç”¨æŒ‡å—", open=False):
                gr.Markdown("""
                ## ğŸš€ å¿«é€Ÿå¼€å§‹

                1. **åŠ è½½æ¨¡å‹**: é€‰æ‹©1.7B(å¿«é€Ÿ)æˆ–17B(é«˜è´¨é‡)æ¨¡å‹ï¼Œç‚¹å‡»"åŠ è½½æ¨¡å‹"
                2. **é€‰æ‹©æ¨¡å¼**: TAæ¨¡å¼æ— éœ€å‚è€ƒå›¾åƒï¼ŒTIAæ¨¡å¼å¯æ§åˆ¶äººç‰©å¤–è§‚
                3. **è¾“å…¥æè¿°**: è¯¦ç»†æè¿°æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹
                4. **ä¸Šä¼ æ–‡ä»¶**: æ ¹æ®éœ€è¦ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶å’Œå‚è€ƒå›¾åƒ
                5. **è°ƒæ•´å‚æ•°**: æ ¹æ®éœ€è¦è°ƒæ•´è§†é¢‘å’Œç”Ÿæˆå‚æ•°
                6. **å¼€å§‹ç”Ÿæˆ**: ç‚¹å‡»ç”ŸæˆæŒ‰é’®ï¼Œç­‰å¾…è§†é¢‘å®Œæˆ

                ## ğŸ’¡ ä¼˜åŒ–å»ºè®®

                **æ–‡æœ¬æç¤ºæŠ€å·§**:
                - ä½¿ç”¨å…·ä½“ã€è¯¦ç»†çš„æè¿°
                - åŒ…å«åŠ¨ä½œã€è¡¨æƒ…ã€ç¯å¢ƒç­‰è¦ç´ 
                - ç¤ºä¾‹ï¼š"ä¸€ä½å¹´è½»å¥³æ€§åœ¨éŸ³ä¹èŠ‚ä¸Šçƒ­æƒ…åœ°è·³èˆï¼Œä¼´éšç€èŠ‚æ‹æŒ¥èˆåŒæ‰‹ï¼Œè„¸ä¸Šæ´‹æº¢ç€å¿«ä¹çš„ç¬‘å®¹"

                **å‚æ•°è°ƒæ•´**:
                - æ–‡æœ¬å¼•å¯¼å¼ºåº¦: æ§åˆ¶å¯¹æ–‡æœ¬æè¿°çš„éµå¾ªåº¦
                - éŸ³é¢‘å¼•å¯¼å¼ºåº¦: æ§åˆ¶åŠ¨ä½œä¸éŸ³é¢‘çš„åŒæ­¥åº¦
                - é‡‡æ ·æ­¥æ•°: 30-50æ­¥é€šå¸¸è¶³å¤Ÿï¼Œæ›´å¤šæ­¥æ•°è´¨é‡æ›´å¥½ä½†æ›´æ…¢

                **æ€§èƒ½ä¼˜åŒ–**:
                - 1.7Bæ¨¡å‹é€‚åˆå¿«é€Ÿæµ‹è¯•
                - 720Pè´¨é‡æ›´å¥½ï¼Œ480Pé€Ÿåº¦æ›´å¿«
                - è°ƒæ•´å¸§æ•°æ§åˆ¶è§†é¢‘é•¿åº¦å’Œç”Ÿæˆæ—¶é—´
                """)

        # äº‹ä»¶å¤„ç†
        def load_model_wrapper(model_type):
            return app.load_model(model_type)

        def get_gpu_status_display():
            """è·å–GPUçŠ¶æ€æ˜¾ç¤º"""
            gpu_status = app.get_gpu_status()
            if gpu_status["available"]:
                return (f"GPU: {gpu_status['device_name']}\n"
                       f"æ˜¾å­˜: {gpu_status['allocated_memory_gb']:.1f}GB / {gpu_status['total_memory_gb']:.1f}GB "
                       f"({gpu_status['utilization']:.1f}% ä½¿ç”¨ç‡)\n"
                       f"å¯ç”¨: {gpu_status['free_memory_gb']:.1f}GB")
            else:
                return gpu_status["message"]

        def optimize_memory_wrapper():
            """å†…å­˜ä¼˜åŒ–åŒ…è£…å‡½æ•°"""
            result = app.optimize_memory()
            # åŒæ—¶è¿”å›ä¼˜åŒ–ç»“æœå’Œæ›´æ–°çš„GPUçŠ¶æ€
            gpu_status = get_gpu_status_display()
            return result, gpu_status

        def generate_video_wrapper(prompt, negative_prompt, ref_image, audio_file, mode, frames, resolution,
                                 scale_i, scale_a, scale_t, sampling_steps, seed, fps, progress=gr.Progress()):
            progress(0, desc="å‡†å¤‡ç”Ÿæˆ...")

            # è§£æåˆ†è¾¨ç‡
            if "720P æ¨ªå±" in resolution:
                height, width = 720, 1280
            elif "720P ç«–å±" in resolution:
                height, width = 1280, 720
            elif "480P æ¨ªå±" in resolution:
                height, width = 480, 832
            elif "480P ç«–å±" in resolution:
                height, width = 832, 480
            elif "1024P æ–¹å½¢" in resolution:
                height, width = 1024, 1024
            else:
                height, width = 720, 1280  # é»˜è®¤å€¼

            # åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°æ¥æ›´æ–°è¿›åº¦
            def progress_tracker():
                while app.is_generating:
                    progress_value = app.update_progress()
                    progress(progress_value/100)
                    time.sleep(0.5)
                    yield progress_value

            # å¯åŠ¨è¿›åº¦æ›´æ–°
            progress_gen = progress_tracker()

            # ç”Ÿæˆè§†é¢‘
            video_path, status, final_progress = app.generate_video(
                prompt, negative_prompt, ref_image, audio_file, mode, frames, height, width,
                scale_i, scale_a, scale_t, sampling_steps, seed, fps
            )

            # è®¾ç½®æœ€ç»ˆè¿›åº¦
            progress(final_progress/100)

            return video_path, status

        # æ¨¡å¼åˆ‡æ¢æ—¶æ˜¾ç¤º/éšè—å‚è€ƒå›¾åƒè¾“å…¥
        def update_ref_image_visibility(mode):
            return gr.update(visible=(mode == "TIA"))

        # å¤„ç†ç¤ºä¾‹æç¤ºé€‰æ‹©
        def load_example_prompt(evt: gr.SelectData):
            return evt.value[0]  # è¿”å›é€‰ä¸­çš„ç¤ºä¾‹æ–‡æœ¬

        # ç»‘å®šäº‹ä»¶
        load_btn.click(
            fn=load_model_wrapper,
            inputs=[model_type],
            outputs=[model_status]
        )

        # GPUçŠ¶æ€æ£€æŸ¥
        gpu_info_btn.click(
            fn=get_gpu_status_display,
            outputs=[gpu_status_display]
        )

        # å†…å­˜ä¼˜åŒ–
        memory_optimize_btn.click(
            fn=optimize_memory_wrapper,
            outputs=[model_status, gpu_status_display]
        )

        mode.change(
            fn=update_ref_image_visibility,
            inputs=[mode],
            outputs=[ref_image]
        )

        example_prompts.select(
            fn=load_example_prompt,
            inputs=[],
            outputs=[prompt]
        )

        generate_btn.click(
            fn=generate_video_wrapper,
            inputs=[prompt, negative_prompt, ref_image, audio_file, mode, frames, resolution,
                   scale_i, scale_a, scale_t, sampling_steps, seed, fps],
            outputs=[output_video, status_text]
        )

    return interface

if __name__ == "__main__":
    # åˆ›å»ºç•Œé¢
    interface = create_interface()

    # å¯åŠ¨åº”ç”¨
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )
